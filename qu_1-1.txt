Quiz 1-1: Back Propagation (20%)
Given an neural network with one hidden layer, and the loss function is cross entropy loss
a = W x + b
h = σ(a)
θ = W h + b
y^ = softmax(θ)
L = CE(y,y^)
x ∈ R 、 W ∈ R 、 b ∈ R 、 h ∈ R 、 W ∈ R 、 b ∈ R 、 θ ∈ R
Calculate L and L
Note: you can write answers on a paper and take a photo of the paper
